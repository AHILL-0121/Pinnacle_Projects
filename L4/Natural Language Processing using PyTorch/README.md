# ğŸ¦ Comparative Sentiment Analysis â€” BERT vs LSTM vs GRU vs RNN (Project 12)

[![Python](https://img.shields.io/badge/Python-3.10+-3776AB?style=for-the-badge&logo=python&logoColor=white)](https://python.org)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-EE4C2C?style=for-the-badge&logo=pytorch&logoColor=white)](https://pytorch.org)
[![HuggingFace](https://img.shields.io/badge/HuggingFace-Transformers-FFD21F?style=for-the-badge&logo=huggingface&logoColor=black)](https://huggingface.co)
[![Jupyter](https://img.shields.io/badge/Jupyter-Notebook-F37626?style=for-the-badge&logo=jupyter&logoColor=white)](https://jupyter.org)
[![Status](https://img.shields.io/badge/Status-Complete-success?style=for-the-badge)]()

*End-to-end NLP notebook that trains and rigorously compares four neural architectures â€” Vanilla RNN, LSTM, GRU, and fine-tuned BERT â€” on the Twitter Sentiment140 dataset, measuring accuracy, F1-score, ROC-AUC, training time, and peak memory consumption side-by-side.*

---

## ğŸ“‹ Table of Contents

- [Overview](#-overview)
- [Dataset](#-dataset)
- [Architecture Comparison](#-architecture-comparison)
- [Model Definitions](#-model-definitions)
- [Workflow](#-workflow)
- [Hyperparameters](#-hyperparameters)
- [Evaluation Metrics](#-evaluation-metrics)
- [Visualizations](#-visualizations)
- [Deployment Recommendations](#-deployment-recommendations)
- [Quick Start](#-quick-start)
- [Dependencies](#-dependencies)
- [File Structure](#-file-structure)

---

## ğŸ¯ Overview

This notebook addresses **binary sentiment classification** (Negative / Positive) on real-world Twitter data. Rather than training a single model, it trains all four architectures on identical data splits and compares them across seven dimensions â€” performance, speed, and resource cost.

| Question Answered | Answer Source |
|-------------------|---------------|
| How much does BERT improve over recurrent models? | Accuracy / F1 comparison table |
| What is the speed-accuracy trade-off for each model? | Training time vs. F1 scatter |
| Which model fits a resource-constrained edge device? | Peak memory + speed chart |
| Where do each model fail? | Normalised confusion matrices |

---

## ğŸ“Š Dataset

| Property | Value |
|----------|-------|
| **Name** | Sentiment140 |
| **Source** | Stanford / Kaggle |
| **Total Records** | 1,600,000 tweets |
| **Sampled for Training** | 6,000 (balanced â€” 3,000 per class) |
| **Classes** | Negative (0), Positive (1) |
| **Split** | 70% Train / 15% Validation / 15% Test |
| **File** | `training.1600000.processed.noemoticon.csv` |

### Text Preprocessing

```
Raw tweet
    â†’ Lowercase
    â†’ Remove URLs  (http/www)
    â†’ Remove @mentions
    â†’ Remove #hashtags
    â†’ Remove non-alphabetic characters & digits
    â†’ Collapse whitespace
    â†’ Filter empty strings after cleaning
```

> **Dataset is NOT bundled** (1.6 GB CSV). Download from  
> [Kaggle â€” Sentiment140](https://www.kaggle.com/datasets/kazanova/sentiment140)  
> and place `training.1600000.processed.noemoticon.csv` in the same folder as the notebook.

---

## ğŸ§  Architecture Comparison

| Attribute | Vanilla RNN | LSTM | GRU | BERT |
|-----------|:-----------:|:----:|:---:|:----:|
| **Architecture** | Recurrent | Recurrent + gates | Recurrent + gates | Transformer |
| **Parameters** | ~2M | ~2M | ~2M | ~110M |
| **Context memory** | Short | Long | Long | Full bidirectional |
| **Vanishing gradient** | Susceptible | Gated (resistant) | Gated (resistant) | Attention (immune) |
| **Pre-training** | âœ— | âœ— | âœ— | âœ“ (`bert-base-uncased`) |
| **GPU speed-up** | Moderate | Moderate | Moderate | Large |
| **Best use case** | Baseline | CPU production | Edge / mobile | High-accuracy server |

---

## ğŸ—ï¸ Model Definitions

### RNN / LSTM / GRU (from scratch)

All three recurrent models share the same skeleton, differing only in the core cell type:

```
Embedding(vocab_size=10000, dim=64, padding_idx=0)
        â”‚
        â–¼
RNN / LSTM / GRU (hidden=128, layers=2, dropout=0.3)
        â”‚  [uses last hidden state]
        â–¼
Dropout(0.3)
        â”‚
        â–¼
Linear(128 â†’ 2)
        â”‚
        â–¼
CrossEntropyLoss
```

| Hyperparameter | Value |
|----------------|-------|
| Vocab Size | 10,000 (top-K words) |
| Max Sequence Length | 50 tokens |
| Embedding Dim | 64 |
| Hidden Dim | 128 |
| RNN Layers | 2 |
| Dropout | 0.3 |
| Optimizer | Adam (lr = 1e-3) |
| Batch Size | 64 |
| Epochs | 5 |

### BERT (fine-tuned)

```
bert-base-uncased (110M params, pre-trained on BooksCorpus + English Wikipedia)
        â”‚  [CLS token representation]
        â–¼
Linear(768 â†’ 2)   [classification head]
        â”‚
        â–¼
CrossEntropyLoss
```

| Hyperparameter | Value |
|----------------|-------|
| Base model | `bert-base-uncased` |
| Max token length | 64 (32 recommended on CPU-only) |
| Batch Size | 32 |
| Optimizer | AdamW (lr = 2e-5, weight_decay = 0.01) |
| Scheduler | Linear warmup (10% of steps) â†’ linear decay |
| Gradient clipping | 1.0 |
| Epochs | 5 |

---

## ğŸ”„ Workflow

```
training.1600000.processed.noemoticon.csv
        â”‚
        â–¼
1. Environment Check (Python, PyTorch, CUDA)
        â”‚
        â–¼
2. Configuration (paths, hyperparameters, SEED=42)
        â”‚
        â–¼
3. Load & Preprocess
   â”œâ”€â”€ Map polarity {0â†’0, 4â†’1}
   â”œâ”€â”€ Clean tweets (regex pipeline)
   â””â”€â”€ Balance + sample (6,000 total)
        â”‚
        â–¼
4. Train/Val/Test Split  (70 / 15 / 15, stratified)
        â”‚
        â–¼
5. Vocabulary Build (top-10K words, <PAD>=0, <UNK>=1)
        â”‚
        â–¼
6. DataLoaders
   â”œâ”€â”€ TextDataset  â†’ integer sequences (RNN/LSTM/GRU)
   â””â”€â”€ BertDataset  â†’ subword token IDs + attention masks
        â”‚
        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼                              â–¼
7a. Train RNN / LSTM / GRU        7b. Train BERT
    (5 epochs each, Adam)             (5 epochs, AdamW + scheduler)
        â”‚                              â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â–¼
8. Comparative Metrics Table (7 columns Ã— 4 models)
        â”‚
        â–¼
9. Visualizations (5 chart files saved to outputs/)
        â”‚
        â–¼
10. Per-Class Classification Reports
        â”‚
        â–¼
11. Final Summary + Deployment Recommendations
```

---

## âš™ï¸ Hyperparameters

| Parameter | RNN / LSTM / GRU | BERT |
|-----------|:----------------:|:----:|
| Epochs | 5 | 5 |
| Batch size | 64 | 32 |
| Learning rate | 1e-3 | 2e-5 |
| Optimizer | Adam | AdamW |
| LR scheduler | None | Linear warmup |
| Max seq len | 50 | 64 |
| Dropout | 0.3 | (internal BERT) |
| Gradient clip | â€” | 1.0 |

---

## ğŸ“ˆ Evaluation Metrics

All metrics are computed on the held-out **test set** (never seen during training):

| Metric | Formula | Description |
|--------|---------|-------------|
| **Accuracy** | $\frac{TP+TN}{N}$ | Overall fraction correct |
| **Precision** | $\frac{TP}{TP+FP}$ | Weighted across classes |
| **Recall** | $\frac{TP}{TP+FN}$ | Weighted across classes |
| **F1-Score** | $2\cdot\frac{P \times R}{P+R}$ | Weighted harmonic mean |
| **ROC-AUC** | Area under ROC curve | Ranking quality (binary: P(pos) used) |
| **Train Time** | seconds | Wall-clock training duration |
| **Peak Memory** | MB | `tracemalloc` peak traced memory |

---

## ğŸ–¼ï¸ Visualizations

All plots are saved to the `outputs/` folder:

| File | Content |
|------|---------|
| `f1_comparison.png` | Bar chart â€” F1-score per model |
| `metrics_comparison.png` | Grouped bar â€” Accuracy, Precision, Recall, F1, ROC-AUC |
| `training_curves.png` | 2Ã—4 grid â€” loss & accuracy curves (train vs. val) per model |
| `confusion_matrices.png` | Normalised confusion matrices for all four models |
| `computational_cost.png` | Training time and peak memory per model |
| `metrics_summary.csv` | Full metrics table exported as CSV |

---

## ğŸš€ Deployment Recommendations

| Scenario | Recommended Model | Rationale |
|----------|:-----------------:|-----------|
| **Production (accuracy critical)** | BERT | Highest accuracy/F1 via contextual embeddings |
| **Edge / mobile / low-latency** | GRU | Best speed-accuracy trade-off among recurrent models |
| **Rapid prototyping / CPU-only** | LSTM or GRU | Fast training, no GPU requirement |
| **Academic baseline** | Vanilla RNN | Simplest architecture, useful lower-bound |

---

## ğŸš€ Quick Start

### 1. Get the dataset

Download from Kaggle and place the CSV in the notebook folder:

```
Natural Language Processing using PyTorch/
â”œâ”€â”€ sentiment_analysis_comparative.ipynb
â”œâ”€â”€ requirements.txt
â””â”€â”€ training.1600000.processed.noemoticon.csv   â† place here
```

### 2. Set up environment

```powershell
cd "L4/Natural Language Processing using PyTorch"
python -m venv venv
.\venv\Scripts\Activate.ps1
pip install -r requirements.txt
```

### 3. (Optional) GPU / CUDA

For significantly faster BERT training, install the CUDA build of PyTorch.  
Visit [pytorch.org/get-started/locally](https://pytorch.org/get-started/locally/) and replace the `torch` line in `requirements.txt` with the matching CUDA wheel URL before running `pip install`.

### 4. Run the notebook

Open `sentiment_analysis_comparative.ipynb` in VS Code or JupyterLab, select the `venv` kernel, then run **Kernel â†’ Restart & Run All**.

> CPU-only estimated runtimes (6,000 sample, 5 epochs):  
> RNN â‰ˆ 30s Â· LSTM â‰ˆ 45s Â· GRU â‰ˆ 40s Â· BERT â‰ˆ 10â€“20 min

---

## ğŸ“¦ Dependencies

```
torch
transformers
scikit-learn
pandas
numpy
matplotlib
seaborn
jupyter
```

Full pinned versions in `requirements.txt`:

```powershell
pip install -r requirements.txt
```

> **Python version:** 3.10+ recommended (3.9+ minimum).

---

## ğŸ“ File Structure

```
Natural Language Processing using PyTorch/
â”œâ”€â”€ Natural Language Processing using PyTorch/
â”‚   â””â”€â”€ sentiment_analysis_comparative.ipynb   # Main notebook (13 sections, 33 cells)
â”œâ”€â”€ requirements.txt                           # Pinned dependencies
â””â”€â”€ README.md                                  # This file
```

### Notebook Sections

| # | Section | Key Output |
|---|---------|-----------|
| 1 | Imports & Environment Check | Python/PyTorch/CUDA versions |
| 2 | Configuration | Paths + hyperparameters |
| 3 | Load & Preprocess Dataset | Balanced 6,000-sample dataframe |
| 4 | Vocabulary (RNN-family) | 10,000-token vocabulary dict |
| 5 | Dataset & DataLoader Classes | `TextDataset`, `BertDataset` |
| 6 | Model Definitions | `SentimentRNN`, `SentimentLSTM`, `SentimentGRU`, `BertSentiment` |
| 7 | Training & Evaluation Helpers | `train_rnn`, `eval_rnn`, `train_bert`, `eval_bert`, `compute_metrics` |
| 8 | Train RNN, LSTM & GRU | Per-epoch loss/accuracy logs |
| 9 | Train BERT | Fine-tuning with ETA progress |
| 10 | Comparative Metrics Table | 7-column summary DataFrame |
| 11 | Visualizations | 5 saved chart files |
| 12 | Per-Class Classification Reports | Precision/Recall/F1 per class |
| 13 | Final Summary & Deployment Recommendations | Best model + use-case guide |

---

## ğŸ“„ License & Author

**License:** MIT â€” Use freely for educational and portfolio purposes.

**Author:** AHILL S
