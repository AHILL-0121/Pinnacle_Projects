{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparative Sentiment Analysis: BERT vs LSTM vs GRU vs RNN\n",
    "### Twitter Sentiment140 Dataset\n",
    "\n",
    "\n",
    "**1. Create & activate a virtual environment:**\n",
    "```bash\n",
    "python -m venv venv\n",
    "\n",
    "# Windows PowerShell\n",
    "venv\\Scripts\\Activate.ps1\n",
    "\n",
    "# Windows CMD\n",
    "venv\\Scripts\\activate.bat\n",
    "\n",
    "# macOS / Linux\n",
    "source venv/bin/activate\n",
    "```\n",
    "\n",
    "**2. Install dependencies:**\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "**3. Select the kernel in VS Code:**\n",
    "- Open this notebook → top-right corner → **Select Kernel** → **Python Environments** → choose `venv`\n",
    "- Or press `Ctrl+Shift+P` → *Notebook: Select Notebook Kernel*\n",
    "\n",
    "**4. Folder layout — place all three files together:**\n",
    "```\n",
    "project/\n",
    "├── sentiment_analysis_comparative.ipynb\n",
    "├── requirements.txt\n",
    "└── training_1600000_processed_noemoticon.csv\n",
    "```\n",
    "\n",
    "> **GPU tip:** For faster BERT training install the CUDA build of PyTorch.\n",
    "> Visit https://pytorch.org/get-started/locally/ and replace the `torch` line in `requirements.txt` with the matching CUDA wheel URL.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports & Environment Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python  : 3.12.3\n",
      "PyTorch : 2.6.0+cu124\n",
      "Device  : cuda  (NVIDIA GeForce RTX 4060 Laptop GPU)\n"
     ]
    }
   ],
   "source": [
    "import os, re, sys, time, tracemalloc, warnings\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'   # suppress HuggingFace fork warning\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')   # non-interactive backend — renders inline in VS Code notebooks\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    precision_recall_fscore_support, confusion_matrix,\n",
    "    roc_auc_score, classification_report\n",
    ")\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import (\n",
    "    BertTokenizer, BertForSequenceClassification,\n",
    "    get_linear_schedule_with_warmup, logging as hf_logging\n",
    ")\n",
    "hf_logging.set_verbosity_error()   # silence HuggingFace INFO/WARNING\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f'Python  : {sys.version.split()[0]}')\n",
    "print(f'PyTorch : {torch.__version__}')\n",
    "print(f'Device  : {DEVICE}', end='')\n",
    "if DEVICE.type == 'cuda':\n",
    "    print(f'  ({torch.cuda.get_device_name(0)})')\n",
    "else:\n",
    "    print('  (CPU only — BERT will be slow; see setup note above)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration — Edit Paths & Hyperparameters Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset  : C:\\Users\\Asus\\Downloads\\Natural Language Processing using PyTorch\\training.1600000.processed.noemoticon.csv\n",
      "Outputs  : C:\\Users\\Asus\\Downloads\\Natural Language Processing using PyTorch\\outputs\n"
     ]
    }
   ],
   "source": [
    "# ── Paths ─────────────────────────────────────────────────────────────────────\n",
    "CSV_PATH   = Path('training.1600000.processed.noemoticon.csv')  # same folder as notebook\n",
    "OUTPUT_DIR = Path('outputs')\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# ── Sampling ──────────────────────────────────────────────────────────────────\n",
    "SAMPLE_SIZE = 6000          # total (balanced); increase for production runs\n",
    "\n",
    "# ── RNN / LSTM / GRU ──────────────────────────────────────────────────────────\n",
    "MAX_VOCAB   = 10_000\n",
    "MAX_SEQ_LEN = 50\n",
    "EMBED_DIM   = 64\n",
    "HIDDEN_DIM  = 128\n",
    "BATCH_SIZE  = 64\n",
    "EPOCHS      = 5\n",
    "LR          = 1e-3\n",
    "\n",
    "# ── BERT ──────────────────────────────────────────────────────────────────────\n",
    "BERT_MODEL   = 'bert-base-uncased'\n",
    "BERT_MAX_LEN = 64           # set to 32 on CPU-only machines for speed\n",
    "BERT_BATCH   = 32\n",
    "BERT_LR      = 2e-5\n",
    "\n",
    "NUM_CLASSES = 2\n",
    "LABEL_NAMES = ['Negative', 'Positive']\n",
    "\n",
    "# num_workers — 0 is safest on Windows; 2-4 on Linux/macOS\n",
    "NW = 0 if sys.platform == 'win32' else 2\n",
    "\n",
    "# ── Validate CSV ──────────────────────────────────────────────────────────────\n",
    "if not CSV_PATH.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Dataset not found: '{CSV_PATH.resolve()}'\\n\"\n",
    "        \"Place 'training.1600000.processed.noemoticon.csv' in the same folder as this notebook.\"\n",
    "    )\n",
    "print(f'Dataset  : {CSV_PATH.resolve()}')\n",
    "print(f'Outputs  : {OUTPUT_DIR.resolve()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load & Preprocess Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw label distribution:\n",
      "label\n",
      "Negative    799996\n",
      "Positive    248576\n",
      "Name: count, dtype: int64\n",
      "Total rows: 1,048,572\n"
     ]
    }
   ],
   "source": [
    "COL_NAMES = ['polarity', 'id', 'date', 'query', 'user', 'text']\n",
    "df = pd.read_csv(CSV_PATH, encoding='latin-1', header=0, names=COL_NAMES)\n",
    "df = df[['polarity', 'text']].copy()\n",
    "\n",
    "# Dataset has polarity 0 (Negative) and 4 (Positive) - binary classification\n",
    "df['label'] = df['polarity'].map({0: 0, 4: 1})\n",
    "df.dropna(subset=['label'], inplace=True)\n",
    "df['label'] = df['label'].astype(int)\n",
    "\n",
    "print('Raw label distribution:')\n",
    "print(df['label'].value_counts().rename(index={0:'Negative', 1:'Positive'}))\n",
    "print(f'Total rows: {len(df):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled distribution (3000 per class):\n",
      "label\n",
      "Negative    3000\n",
      "Positive    3000\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sun is amazing but still havent got a sutan x</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>new favorite store bed bath amp beyond so many...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>studying for examsss yuck</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          clean_text  label\n",
       "0      sun is amazing but still havent got a sutan x      0\n",
       "1  new favorite store bed bath amp beyond so many...      0\n",
       "2                          studying for examsss yuck      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Lowercase; strip URLs, @mentions, #hashtags, special chars & digits.\"\"\"\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'#\\w+', '', text)\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "df['clean_text'] = df['text'].apply(clean_text)\n",
    "df = df[df['clean_text'].str.strip() != '']\n",
    "\n",
    "per_class = SAMPLE_SIZE // df['label'].nunique()\n",
    "\n",
    "# Sample balanced classes - compatible with pandas 2.1+\n",
    "df_sampled = pd.concat([\n",
    "    df[df['label'] == label].sample(min(len(df[df['label'] == label]), per_class), random_state=SEED)\n",
    "    for label in df['label'].unique()\n",
    "]).reset_index(drop=True)\n",
    "\n",
    "print(f'Sampled distribution ({per_class} per class):')\n",
    "print(df_sampled['label'].value_counts().sort_index().rename(index={0:'Negative', 1:'Positive'}))\n",
    "df_sampled[['clean_text', 'label']].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 4,200  |  Val: 900  |  Test: 900\n"
     ]
    }
   ],
   "source": [
    "texts  = df_sampled['clean_text'].tolist()\n",
    "labels = df_sampled['label'].tolist()\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    texts, labels, test_size=0.30, random_state=SEED, stratify=labels)\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.50, random_state=SEED, stratify=y_temp)\n",
    "\n",
    "print(f'Train: {len(X_train):,}  |  Val: {len(X_val):,}  |  Test: {len(X_test):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Vocabulary (RNN-family)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 8,133\n"
     ]
    }
   ],
   "source": [
    "def build_vocab(corpus, max_vocab=MAX_VOCAB):\n",
    "    counter = Counter(token for sent in corpus for token in sent.split())\n",
    "    vocab   = {'<PAD>': 0, '<UNK>': 1}\n",
    "    for word, _ in counter.most_common(max_vocab - 2):\n",
    "        vocab[word] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "def encode(text, vocab, max_len=MAX_SEQ_LEN):\n",
    "    ids = [vocab.get(t, 1) for t in text.split()[:max_len]]\n",
    "    return ids + [0] * (max_len - len(ids))\n",
    "\n",
    "vocab      = build_vocab(X_train)\n",
    "VOCAB_SIZE = len(vocab)\n",
    "print(f'Vocabulary size: {VOCAB_SIZE:,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dataset & DataLoader Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoaders ready.\n"
     ]
    }
   ],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, vocab):\n",
    "        self.X = torch.tensor([encode(t, vocab) for t in texts], dtype=torch.long)\n",
    "        self.y = torch.tensor(labels, dtype=torch.long)\n",
    "    def __len__(self):        return len(self.y)\n",
    "    def __getitem__(self, i): return self.X[i], self.y[i]\n",
    "\n",
    "\n",
    "class BertDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=BERT_MAX_LEN):\n",
    "        enc = tokenizer(texts, padding='max_length', truncation=True,\n",
    "                        max_length=max_len, return_tensors='pt')\n",
    "        self.input_ids      = enc['input_ids']\n",
    "        self.attention_mask = enc['attention_mask']\n",
    "        self.labels         = torch.tensor(labels, dtype=torch.long)\n",
    "    def __len__(self):        return len(self.labels)\n",
    "    def __getitem__(self, i): return self.input_ids[i], self.attention_mask[i], self.labels[i]\n",
    "\n",
    "\n",
    "PIN = DEVICE.type == 'cuda'\n",
    "\n",
    "train_loader = DataLoader(TextDataset(X_train, y_train, vocab),\n",
    "                          batch_size=BATCH_SIZE, shuffle=True,  num_workers=NW, pin_memory=PIN)\n",
    "val_loader   = DataLoader(TextDataset(X_val,   y_val,   vocab),\n",
    "                          batch_size=BATCH_SIZE, shuffle=False, num_workers=NW, pin_memory=PIN)\n",
    "test_loader  = DataLoader(TextDataset(X_test,  y_test,  vocab),\n",
    "                          batch_size=BATCH_SIZE, shuffle=False, num_workers=NW, pin_memory=PIN)\n",
    "\n",
    "print('DataLoaders ready.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model classes defined.\n"
     ]
    }
   ],
   "source": [
    "class SentimentRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.emb  = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.rnn  = nn.RNN(embed_dim, hidden_dim, num_layers=2,\n",
    "                           batch_first=True, dropout=dropout)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.fc   = nn.Linear(hidden_dim, num_classes)\n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(self.emb(x))\n",
    "        return self.fc(self.drop(out[:, -1, :]))\n",
    "\n",
    "\n",
    "class SentimentLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.emb  = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers=2,\n",
    "                            batch_first=True, dropout=dropout)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.fc   = nn.Linear(hidden_dim, num_classes)\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(self.emb(x))\n",
    "        return self.fc(self.drop(out[:, -1, :]))\n",
    "\n",
    "\n",
    "class SentimentGRU(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.emb  = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.gru  = nn.GRU(embed_dim, hidden_dim, num_layers=2,\n",
    "                           batch_first=True, dropout=dropout)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.fc   = nn.Linear(hidden_dim, num_classes)\n",
    "    def forward(self, x):\n",
    "        out, _ = self.gru(self.emb(x))\n",
    "        return self.fc(self.drop(out[:, -1, :]))\n",
    "\n",
    "\n",
    "class BertSentiment(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.bert = BertForSequenceClassification.from_pretrained(\n",
    "            BERT_MODEL, num_labels=num_classes,\n",
    "            ignore_mismatched_sizes=True  # suppresses the LOAD REPORT noise\n",
    "        )\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        return self.bert(input_ids=input_ids, attention_mask=attention_mask).logits\n",
    "\n",
    "\n",
    "print('Model classes defined.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training & Evaluation Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions defined.\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "def train_rnn(model, loader, opt):\n",
    "    model.train()\n",
    "    loss_sum = correct = total = 0\n",
    "    for X, y in loader:\n",
    "        X, y = X.to(DEVICE), y.to(DEVICE)\n",
    "        opt.zero_grad()\n",
    "        out  = model(X)\n",
    "        loss = criterion(out, y)\n",
    "        loss.backward(); opt.step()\n",
    "        loss_sum += loss.item() * len(y)\n",
    "        correct  += (out.argmax(1) == y).sum().item()\n",
    "        total    += len(y)\n",
    "    return loss_sum / total, correct / total\n",
    "\n",
    "\n",
    "def eval_rnn(model, loader):\n",
    "    model.eval()\n",
    "    loss_sum = correct = total = 0\n",
    "    preds_all, labels_all, probs_all = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for X, y in loader:\n",
    "            X, y  = X.to(DEVICE), y.to(DEVICE)\n",
    "            out   = model(X)\n",
    "            probs = torch.softmax(out, 1)\n",
    "            pred  = probs.argmax(1)\n",
    "            loss_sum += criterion(out, y).item() * len(y)\n",
    "            correct  += (pred == y).sum().item()\n",
    "            total    += len(y)\n",
    "            preds_all.extend(pred.cpu().tolist())\n",
    "            labels_all.extend(y.cpu().tolist())\n",
    "            probs_all.extend(probs.cpu().numpy())\n",
    "    return loss_sum/total, correct/total, preds_all, labels_all, np.array(probs_all)\n",
    "\n",
    "\n",
    "def train_bert(model, loader, opt, sch):\n",
    "    model.train()\n",
    "    loss_sum = correct = total = 0\n",
    "    for ids, mask, y in loader:\n",
    "        ids, mask, y = ids.to(DEVICE), mask.to(DEVICE), y.to(DEVICE)\n",
    "        opt.zero_grad()\n",
    "        out  = model(ids, mask)\n",
    "        loss = criterion(out, y)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        opt.step(); sch.step()\n",
    "        loss_sum += loss.item() * len(y)\n",
    "        correct  += (out.argmax(1) == y).sum().item()\n",
    "        total    += len(y)\n",
    "    return loss_sum/total, correct/total\n",
    "\n",
    "\n",
    "def eval_bert(model, loader):\n",
    "    model.eval()\n",
    "    loss_sum = correct = total = 0\n",
    "    preds_all, labels_all, probs_all = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for ids, mask, y in loader:\n",
    "            ids, mask, y = ids.to(DEVICE), mask.to(DEVICE), y.to(DEVICE)\n",
    "            out   = model(ids, mask)\n",
    "            probs = torch.softmax(out, 1)\n",
    "            pred  = probs.argmax(1)\n",
    "            loss_sum += criterion(out, y).item() * len(y)\n",
    "            correct  += (pred == y).sum().item()\n",
    "            total    += len(y)\n",
    "            preds_all.extend(pred.cpu().tolist())\n",
    "            labels_all.extend(y.cpu().tolist())\n",
    "            probs_all.extend(probs.cpu().numpy())\n",
    "    return loss_sum/total, correct/total, preds_all, labels_all, np.array(probs_all)\n",
    "\n",
    "\n",
    "def compute_metrics(true, preds, probs):\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "        true, preds, average='weighted', zero_division=0)\n",
    "    # For binary classification, use probability of positive class (column 1)\n",
    "    if NUM_CLASSES == 2:\n",
    "        roc = roc_auc_score(true, probs[:, 1]) if len(np.unique(true)) == NUM_CLASSES else float('nan')\n",
    "    else:\n",
    "        roc = roc_auc_score(true, probs, multi_class='ovr', average='weighted') \\\n",
    "              if len(np.unique(true)) == NUM_CLASSES else float('nan')\n",
    "    return prec, rec, f1, roc\n",
    "\n",
    "\n",
    "print('Helper functions defined.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Train RNN, LSTM & GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "  Training RNN\n",
      "============================================================\n",
      "  Epoch 1/5  TrainLoss=0.6979  TrainAcc=0.4771  ValLoss=0.6937  ValAcc=0.5000\n",
      "  Epoch 2/5  TrainLoss=0.6950  TrainAcc=0.4943  ValLoss=0.6932  ValAcc=0.5000\n",
      "  Epoch 3/5  TrainLoss=0.6939  TrainAcc=0.4876  ValLoss=0.6932  ValAcc=0.5000\n",
      "  Epoch 4/5  TrainLoss=0.6941  TrainAcc=0.5083  ValLoss=0.6934  ValAcc=0.4956\n",
      "  Epoch 5/5  TrainLoss=0.7110  TrainAcc=0.5029  ValLoss=0.7055  ValAcc=0.4878\n",
      "  ✓ RNN  Acc=0.4967  F1=0.4897  ROC-AUC=0.4967  Time=2.1s  Mem=0.2MB\n",
      "\n",
      "============================================================\n",
      "  Training LSTM\n",
      "============================================================\n",
      "  Epoch 1/5  TrainLoss=0.6938  TrainAcc=0.5024  ValLoss=0.6939  ValAcc=0.5000\n",
      "  Epoch 2/5  TrainLoss=0.6937  TrainAcc=0.4867  ValLoss=0.6932  ValAcc=0.5000\n",
      "  Epoch 3/5  TrainLoss=0.6936  TrainAcc=0.4967  ValLoss=0.6932  ValAcc=0.5000\n",
      "  Epoch 4/5  TrainLoss=0.6935  TrainAcc=0.5050  ValLoss=0.6935  ValAcc=0.5000\n",
      "  Epoch 5/5  TrainLoss=0.6938  TrainAcc=0.4900  ValLoss=0.6932  ValAcc=0.5000\n",
      "  ✓ LSTM  Acc=0.5000  F1=0.3333  ROC-AUC=0.4933  Time=2.2s  Mem=0.2MB\n",
      "\n",
      "============================================================\n",
      "  Training GRU\n",
      "============================================================\n",
      "  Epoch 1/5  TrainLoss=0.6952  TrainAcc=0.4855  ValLoss=0.6936  ValAcc=0.5000\n",
      "  Epoch 2/5  TrainLoss=0.6945  TrainAcc=0.4874  ValLoss=0.6932  ValAcc=0.5000\n",
      "  Epoch 3/5  TrainLoss=0.6936  TrainAcc=0.5079  ValLoss=0.6955  ValAcc=0.5000\n",
      "  Epoch 4/5  TrainLoss=0.6940  TrainAcc=0.4924  ValLoss=0.6932  ValAcc=0.5000\n",
      "  Epoch 5/5  TrainLoss=0.6932  TrainAcc=0.5040  ValLoss=0.6935  ValAcc=0.5000\n",
      "  ✓ GRU  Acc=0.5000  F1=0.3333  ROC-AUC=0.4838  Time=2.0s  Mem=0.2MB\n",
      "\n",
      "RNN-family training complete.\n"
     ]
    }
   ],
   "source": [
    "results     = {}\n",
    "model_order = ['RNN', 'LSTM', 'GRU', 'BERT']\n",
    "colors      = ['#e74c3c', '#3498db', '#2ecc71', '#9b59b6']\n",
    "\n",
    "for name, ModelCls in [('RNN', SentimentRNN), ('LSTM', SentimentLSTM), ('GRU', SentimentGRU)]:\n",
    "    print(f'\\n{\"=\"*60}\\n  Training {name}\\n{\"=\"*60}')\n",
    "\n",
    "    model = ModelCls(VOCAB_SIZE, EMBED_DIM, HIDDEN_DIM, NUM_CLASSES).to(DEVICE)\n",
    "    opt   = optim.Adam(model.parameters(), lr=LR)\n",
    "    hist  = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
    "\n",
    "    tracemalloc.start()\n",
    "    t0 = time.time()\n",
    "\n",
    "    for ep in range(1, EPOCHS + 1):\n",
    "        tl, ta = train_rnn(model, train_loader, opt)\n",
    "        vl, va, _, _, _ = eval_rnn(model, val_loader)\n",
    "        hist['train_loss'].append(tl); hist['val_loss'].append(vl)\n",
    "        hist['train_acc'].append(ta);  hist['val_acc'].append(va)\n",
    "        print(f'  Epoch {ep}/{EPOCHS}  TrainLoss={tl:.4f}  TrainAcc={ta:.4f}  '\n",
    "              f'ValLoss={vl:.4f}  ValAcc={va:.4f}')\n",
    "\n",
    "    train_time = time.time() - t0\n",
    "    mem_mb     = tracemalloc.get_traced_memory()[1] / 1e6\n",
    "    tracemalloc.stop()\n",
    "\n",
    "    _, acc, preds, true, probs = eval_rnn(model, test_loader)\n",
    "    prec, rec, f1, roc         = compute_metrics(true, preds, probs)\n",
    "\n",
    "    results[name] = dict(history=hist, accuracy=acc, precision=prec, recall=rec,\n",
    "                         f1=f1, roc_auc=roc, cm=confusion_matrix(true, preds),\n",
    "                         preds=preds, true=true, train_time=train_time, mem_mb=mem_mb)\n",
    "\n",
    "    print(f'  ✓ {name}  Acc={acc:.4f}  F1={f1:.4f}  ROC-AUC={roc:.4f}  '\n",
    "          f'Time={train_time:.1f}s  Mem={mem_mb:.1f}MB')\n",
    "\n",
    "print('\\nRNN-family training complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Train BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "  Training BERT (longest step — ETA shown per epoch)\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7fc2660756444238477727ca4e35e31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a18dcc836df14377823467af878dd408",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11d169dedd6d42a9bc513e3a4b18e467",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a027da34a2064b32b33ef87098d2913c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31d86942b1614916b6a6b4a17b7440da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d7b1bca3813473087f7e9c9e27680a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 1/5  TrainLoss=0.5697  TrainAcc=0.6957  ValLoss=0.4966  ValAcc=0.7744  Elapsed=29s  ETA≈115s\n",
      "  Epoch 2/5  TrainLoss=0.3673  TrainAcc=0.8443  ValLoss=0.4408  ValAcc=0.8178  Elapsed=57s  ETA≈85s\n",
      "  Epoch 3/5  TrainLoss=0.2369  TrainAcc=0.9069  ValLoss=0.5022  ValAcc=0.8000  Elapsed=85s  ETA≈57s\n",
      "  Epoch 4/5  TrainLoss=0.1398  TrainAcc=0.9569  ValLoss=0.6013  ValAcc=0.8011  Elapsed=114s  ETA≈28s\n",
      "  Epoch 5/5  TrainLoss=0.0878  TrainAcc=0.9736  ValLoss=0.6390  ValAcc=0.8156  Elapsed=142s  ETA≈0s\n",
      "  ✓ BERT  Acc=0.8322  F1=0.8322  ROC-AUC=0.8930  Time=141.8s  Mem=0.6MB\n"
     ]
    }
   ],
   "source": [
    "print(f'\\n{\"=\"*60}\\n  Training BERT (longest step — ETA shown per epoch)\\n{\"=\"*60}')\n",
    "\n",
    "bert_tokenizer    = BertTokenizer.from_pretrained(BERT_MODEL)\n",
    "bert_train_loader = DataLoader(BertDataset(X_train, y_train, bert_tokenizer),\n",
    "                               batch_size=BERT_BATCH, shuffle=True,  num_workers=NW, pin_memory=PIN)\n",
    "bert_val_loader   = DataLoader(BertDataset(X_val,   y_val,   bert_tokenizer),\n",
    "                               batch_size=BERT_BATCH, shuffle=False, num_workers=NW, pin_memory=PIN)\n",
    "bert_test_loader  = DataLoader(BertDataset(X_test,  y_test,  bert_tokenizer),\n",
    "                               batch_size=BERT_BATCH, shuffle=False, num_workers=NW, pin_memory=PIN)\n",
    "\n",
    "bert_model  = BertSentiment(NUM_CLASSES).to(DEVICE)\n",
    "bert_opt    = optim.AdamW(bert_model.parameters(), lr=BERT_LR, weight_decay=0.01)\n",
    "total_steps = len(bert_train_loader) * EPOCHS\n",
    "scheduler   = get_linear_schedule_with_warmup(\n",
    "    bert_opt, num_warmup_steps=int(0.1 * total_steps),\n",
    "    num_training_steps=total_steps)\n",
    "\n",
    "hist = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
    "\n",
    "tracemalloc.start()\n",
    "t0 = time.time()\n",
    "\n",
    "for ep in range(1, EPOCHS + 1):\n",
    "    tl, ta = train_bert(bert_model, bert_train_loader, bert_opt, scheduler)\n",
    "    vl, va, _, _, _ = eval_bert(bert_model, bert_val_loader)\n",
    "    hist['train_loss'].append(tl); hist['val_loss'].append(vl)\n",
    "    hist['train_acc'].append(ta);  hist['val_acc'].append(va)\n",
    "    elapsed = time.time() - t0\n",
    "    eta     = elapsed / ep * (EPOCHS - ep)\n",
    "    print(f'  Epoch {ep}/{EPOCHS}  TrainLoss={tl:.4f}  TrainAcc={ta:.4f}  '\n",
    "          f'ValLoss={vl:.4f}  ValAcc={va:.4f}  '\n",
    "          f'Elapsed={elapsed:.0f}s  ETA≈{eta:.0f}s')\n",
    "\n",
    "bert_time = time.time() - t0\n",
    "bert_mem  = tracemalloc.get_traced_memory()[1] / 1e6\n",
    "tracemalloc.stop()\n",
    "\n",
    "_, acc, preds, true, probs = eval_bert(bert_model, bert_test_loader)\n",
    "prec, rec, f1, roc         = compute_metrics(true, preds, probs)\n",
    "\n",
    "results['BERT'] = dict(history=hist, accuracy=acc, precision=prec, recall=rec,\n",
    "                       f1=f1, roc_auc=roc, cm=confusion_matrix(true, preds),\n",
    "                       preds=preds, true=true, train_time=bert_time, mem_mb=bert_mem)\n",
    "\n",
    "print(f'  ✓ BERT  Acc={acc:.4f}  F1={f1:.4f}  ROC-AUC={roc:.4f}  '\n",
    "      f'Time={bert_time:.1f}s  Mem={bert_mem:.1f}MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Comparative Metrics Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Comparative Metrics Summary ===\n",
      "      Accuracy  Precision  Recall  F1-Score  ROC-AUC  Train Time (s)  Peak Mem (MB)\n",
      "RNN     0.4967     0.4965  0.4967    0.4897   0.4967          2.1091         0.2185\n",
      "LSTM    0.5000     0.2500  0.5000    0.3333   0.4933          2.2058         0.2078\n",
      "GRU     0.5000     0.2500  0.5000    0.3333   0.4838          2.0053         0.2115\n",
      "BERT    0.8322     0.8323  0.8322    0.8322   0.8930        141.8056         0.6118\n",
      "\n",
      "Saved → outputs\\metrics_summary.csv\n"
     ]
    }
   ],
   "source": [
    "summary = pd.DataFrame({\n",
    "    'Accuracy':       [results[m]['accuracy']   for m in model_order],\n",
    "    'Precision':      [results[m]['precision']  for m in model_order],\n",
    "    'Recall':         [results[m]['recall']     for m in model_order],\n",
    "    'F1-Score':       [results[m]['f1']         for m in model_order],\n",
    "    'ROC-AUC':        [results[m]['roc_auc']    for m in model_order],\n",
    "    'Train Time (s)': [results[m]['train_time'] for m in model_order],\n",
    "    'Peak Mem (MB)':  [results[m]['mem_mb']     for m in model_order],\n",
    "}, index=model_order)\n",
    "\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "print('\\n=== Comparative Metrics Summary ===')\n",
    "print(summary.to_string())\n",
    "\n",
    "summary.to_csv(OUTPUT_DIR / 'metrics_summary.csv')\n",
    "print(f'\\nSaved → {OUTPUT_DIR / \"metrics_summary.csv\"}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved → outputs\\f1_comparison.png\n"
     ]
    }
   ],
   "source": [
    "# ── 11a. F1 bar chart ─────────────────────────────────────────────────────────\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "f1s  = [results[m]['f1'] for m in model_order]\n",
    "bars = ax.bar(model_order, f1s, color=colors, edgecolor='white', width=0.5)\n",
    "for b, v in zip(bars, f1s):\n",
    "    ax.text(b.get_x() + b.get_width()/2, b.get_height() + 0.005,\n",
    "            f'{v:.4f}', ha='center', fontsize=11, fontweight='bold')\n",
    "ax.set_ylim(0, 1.1); ax.set_ylabel('Weighted F1-Score', fontsize=12)\n",
    "ax.set_title('F1-Score Comparison', fontsize=14, fontweight='bold')\n",
    "ax.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "fig.savefig(OUTPUT_DIR / 'f1_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show(); print(f'Saved → {OUTPUT_DIR / \"f1_comparison.png\"}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved → outputs\\metrics_comparison.png\n"
     ]
    }
   ],
   "source": [
    "# ── 11b. All metrics grouped bar ──────────────────────────────────────────────\n",
    "met_keys = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n",
    "met_lbls = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']\n",
    "x, w = np.arange(len(met_keys)), 0.18\n",
    "fig, ax = plt.subplots(figsize=(13, 6))\n",
    "for i, (m, c) in enumerate(zip(model_order, colors)):\n",
    "    ax.bar(x + i*w - 1.5*w, [results[m][k] for k in met_keys],\n",
    "           w, label=m, color=c, edgecolor='white', alpha=0.9)\n",
    "ax.set_xticks(x); ax.set_xticklabels(met_lbls, fontsize=11)\n",
    "ax.set_ylim(0, 1.1); ax.set_ylabel('Score', fontsize=12)\n",
    "ax.set_title('All Performance Metrics', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11); ax.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "fig.savefig(OUTPUT_DIR / 'metrics_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show(); print(f'Saved → {OUTPUT_DIR / \"metrics_comparison.png\"}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved → outputs\\training_curves.png\n"
     ]
    }
   ],
   "source": [
    "# ── 11c. Training / validation curves ────────────────────────────────────────\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 9))\n",
    "ep = range(1, EPOCHS + 1)\n",
    "for col, (m, c) in enumerate(zip(model_order, colors)):\n",
    "    h = results[m]['history']\n",
    "    for row, (tr_k, vl_k, ylabel) in enumerate([\n",
    "        ('train_loss', 'val_loss', 'Loss'),\n",
    "        ('train_acc',  'val_acc',  'Accuracy')\n",
    "    ]):\n",
    "        ax = axes[row][col]\n",
    "        ax.plot(ep, h[tr_k], 'o-',  color=c, label='Train', lw=2)\n",
    "        ax.plot(ep, h[vl_k], 's--', color=c, label='Val',   lw=2, alpha=0.7)\n",
    "        ax.set_title(f'{m} – {ylabel}', fontweight='bold')\n",
    "        ax.set_xlabel('Epoch'); ax.set_ylabel(ylabel)\n",
    "        if ylabel == 'Accuracy': ax.set_ylim(0, 1)\n",
    "        ax.legend(); ax.grid(alpha=0.3)\n",
    "        ax.xaxis.set_major_locator(ticker.MaxNLocator(integer=True))\n",
    "plt.suptitle('Training & Validation Curves', fontsize=15, fontweight='bold', y=1.01)\n",
    "plt.tight_layout()\n",
    "fig.savefig(OUTPUT_DIR / 'training_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show(); print(f'Saved → {OUTPUT_DIR / \"training_curves.png\"}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved → outputs\\confusion_matrices.png\n"
     ]
    }
   ],
   "source": [
    "# ── 11d. Confusion matrices ───────────────────────────────────────────────────\n",
    "fig, axes = plt.subplots(1, 4, figsize=(22, 5))\n",
    "for ax, m in zip(axes, model_order):\n",
    "    cm   = results[m]['cm'].astype(float)\n",
    "    cm_n = cm / cm.sum(axis=1, keepdims=True)\n",
    "    sns.heatmap(cm_n, annot=True, fmt='.2f', cmap='Blues',\n",
    "                xticklabels=LABEL_NAMES, yticklabels=LABEL_NAMES,\n",
    "                ax=ax, linewidths=0.5, linecolor='white', vmin=0, vmax=1)\n",
    "    ax.set_title(f\"{m}\\nAcc={results[m]['accuracy']:.3f}\", fontweight='bold')\n",
    "    ax.set_xlabel('Predicted'); ax.set_ylabel('True')\n",
    "plt.suptitle('Normalised Confusion Matrices', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "fig.savefig(OUTPUT_DIR / 'confusion_matrices.png', dpi=150, bbox_inches='tight')\n",
    "plt.show(); print(f'Saved → {OUTPUT_DIR / \"confusion_matrices.png\"}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved → outputs\\computational_cost.png\n"
     ]
    }
   ],
   "source": [
    "# ── 11e. Computational cost ───────────────────────────────────────────────────\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "for ax, vals, ylabel, unit in [\n",
    "    (ax1, [results[m]['train_time'] for m in model_order], 'Training Time', 's'),\n",
    "    (ax2, [results[m]['mem_mb']     for m in model_order], 'Peak Memory',   'MB')\n",
    "]:\n",
    "    bars = ax.bar(model_order, vals, color=colors, edgecolor='white')\n",
    "    for b, v in zip(bars, vals):\n",
    "        ax.text(b.get_x() + b.get_width()/2, b.get_height() + max(vals)*0.01,\n",
    "                f'{v:.1f}{unit}', ha='center', fontsize=10, fontweight='bold')\n",
    "    ax.set_title(f'{ylabel} ({unit})', fontweight='bold')\n",
    "    ax.set_ylabel(unit); ax.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "plt.suptitle('Computational Requirements', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "fig.savefig(OUTPUT_DIR / 'computational_cost.png', dpi=150, bbox_inches='tight')\n",
    "plt.show(); print(f'Saved → {OUTPUT_DIR / \"computational_cost.png\"}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Per-Class Classification Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "───────────────────────────────────────────────────────\n",
      "  Classification Report — RNN\n",
      "───────────────────────────────────────────────────────\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.50      0.61      0.55       450\n",
      "    Positive       0.50      0.38      0.43       450\n",
      "\n",
      "    accuracy                           0.50       900\n",
      "   macro avg       0.50      0.50      0.49       900\n",
      "weighted avg       0.50      0.50      0.49       900\n",
      "\n",
      "\n",
      "───────────────────────────────────────────────────────\n",
      "  Classification Report — LSTM\n",
      "───────────────────────────────────────────────────────\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.00      0.00      0.00       450\n",
      "    Positive       0.50      1.00      0.67       450\n",
      "\n",
      "    accuracy                           0.50       900\n",
      "   macro avg       0.25      0.50      0.33       900\n",
      "weighted avg       0.25      0.50      0.33       900\n",
      "\n",
      "\n",
      "───────────────────────────────────────────────────────\n",
      "  Classification Report — GRU\n",
      "───────────────────────────────────────────────────────\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.00      0.00      0.00       450\n",
      "    Positive       0.50      1.00      0.67       450\n",
      "\n",
      "    accuracy                           0.50       900\n",
      "   macro avg       0.25      0.50      0.33       900\n",
      "weighted avg       0.25      0.50      0.33       900\n",
      "\n",
      "\n",
      "───────────────────────────────────────────────────────\n",
      "  Classification Report — BERT\n",
      "───────────────────────────────────────────────────────\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.83      0.84      0.83       450\n",
      "    Positive       0.84      0.82      0.83       450\n",
      "\n",
      "    accuracy                           0.83       900\n",
      "   macro avg       0.83      0.83      0.83       900\n",
      "weighted avg       0.83      0.83      0.83       900\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for m in model_order:\n",
    "    print(f'\\n{\"─\"*55}\\n  Classification Report — {m}\\n{\"─\"*55}')\n",
    "    print(classification_report(results[m]['true'], results[m]['preds'],\n",
    "                                target_names=LABEL_NAMES, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Final Summary & Deployment Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=================================================================\n",
      "  FINAL COMPARATIVE SUMMARY\n",
      "=================================================================\n",
      "      Accuracy  Precision  Recall  F1-Score  ROC-AUC  Train Time (s)  Peak Mem (MB)\n",
      "RNN     0.4967     0.4965  0.4967    0.4897   0.4967          2.1091         0.2185\n",
      "LSTM    0.5000     0.2500  0.5000    0.3333   0.4933          2.2058         0.2078\n",
      "GRU     0.5000     0.2500  0.5000    0.3333   0.4838          2.0053         0.2115\n",
      "BERT    0.8322     0.8323  0.8322    0.8322   0.8930        141.8056         0.6118\n",
      "\n",
      "  Best Accuracy  : BERT  (0.8322)\n",
      "  Best F1-Score  : BERT  (0.8322)\n",
      "  Fastest Train  : GRU  (2.0s)\n",
      "\n",
      "─────────────────────────────────────────────────────────────────\n",
      "INSIGHTS\n",
      "─────────────────────────────────────────────────────────────────\n",
      "BERT (110M-param transformer) achieves the best accuracy/F1\n",
      "thanks to deep contextual embeddings, but costs significantly\n",
      "more time and memory.\n",
      "\n",
      "GRU and LSTM outperform vanilla RNN via gating mechanisms that\n",
      "prevent vanishing gradients on longer sequences. GRU trains\n",
      "slightly faster than LSTM with similar accuracy.\n",
      "\n",
      "Vanilla RNN is the weakest baseline and degrades on tweets\n",
      "with longer context.\n",
      "\n",
      "DEPLOYMENT RECOMMENDATIONS\n",
      "─────────────────────────────────────────────────────────────────\n",
      "  Production (accuracy critical)    →  Fine-tuned BERT\n",
      "  Edge / mobile / low-latency       →  GRU  (best speed-accuracy trade-off)\n",
      "  Rapid prototyping / CPU only      →  LSTM or GRU\n",
      "  Academic baseline                 →  RNN\n",
      "─────────────────────────────────────────────────────────────────\n",
      "\n",
      "All outputs saved to: C:\\Users\\Asus\\Downloads\\Natural Language Processing using PyTorch\\outputs\n"
     ]
    }
   ],
   "source": [
    "print('\\n' + '='*65 + '\\n  FINAL COMPARATIVE SUMMARY\\n' + '='*65)\n",
    "print(summary.to_string())\n",
    "\n",
    "best_acc = summary['Accuracy'].idxmax()\n",
    "best_f1  = summary['F1-Score'].idxmax()\n",
    "fastest  = summary['Train Time (s)'].idxmin()\n",
    "\n",
    "print(f'\\n  Best Accuracy  : {best_acc}  ({summary.loc[best_acc, \"Accuracy\"]:.4f})')\n",
    "print(f'  Best F1-Score  : {best_f1}  ({summary.loc[best_f1, \"F1-Score\"]:.4f})')\n",
    "print(f'  Fastest Train  : {fastest}  ({summary.loc[fastest, \"Train Time (s)\"]:.1f}s)')\n",
    "\n",
    "print('''\n",
    "─────────────────────────────────────────────────────────────────\n",
    "INSIGHTS\n",
    "─────────────────────────────────────────────────────────────────\n",
    "BERT (110M-param transformer) achieves the best accuracy/F1\n",
    "thanks to deep contextual embeddings, but costs significantly\n",
    "more time and memory.\n",
    "\n",
    "GRU and LSTM outperform vanilla RNN via gating mechanisms that\n",
    "prevent vanishing gradients on longer sequences. GRU trains\n",
    "slightly faster than LSTM with similar accuracy.\n",
    "\n",
    "Vanilla RNN is the weakest baseline and degrades on tweets\n",
    "with longer context.\n",
    "\n",
    "DEPLOYMENT RECOMMENDATIONS\n",
    "─────────────────────────────────────────────────────────────────\n",
    "  Production (accuracy critical)    →  Fine-tuned BERT\n",
    "  Edge / mobile / low-latency       →  GRU  (best speed-accuracy trade-off)\n",
    "  Rapid prototyping / CPU only      →  LSTM or GRU\n",
    "  Academic baseline                 →  RNN\n",
    "─────────────────────────────────────────────────────────────────\n",
    "''')\n",
    "\n",
    "print(f'All outputs saved to: {OUTPUT_DIR.resolve()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
