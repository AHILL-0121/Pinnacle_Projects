<div align="center">

# ğŸ”ï¸ Pinnacle Projects

### A Portfolio of Production-Grade AI & Financial Engineering Systems

[![Python](https://img.shields.io/badge/Python-3.10+-3776AB?style=for-the-badge&logo=python&logoColor=white)](https://python.org)
[![Flask](https://img.shields.io/badge/Flask-3.0+-000000?style=for-the-badge&logo=flask&logoColor=white)](https://flask.palletsprojects.com)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.109-009688?style=for-the-badge&logo=fastapi&logoColor=white)](https://fastapi.tiangolo.com)
[![LLM](https://img.shields.io/badge/LLM-Multi--Provider-purple?style=for-the-badge)]()
[![License](https://img.shields.io/badge/License-MIT-green?style=for-the-badge)](LICENSE)

*Four interconnected projects spanning REST API design, OCR-powered document intelligence, retrieval-augmented generation, and autonomous AI agents â€” each built from scratch with enterprise-grade architecture.*

</div>

---

## ğŸ“‹ Table of Contents

- [Repository Overview](#-repository-overview)
- [Architecture Map](#-architecture-map)
- [Project 1 â€” Flask Stock Intelligence API](#-project-1--flask-stock-intelligence-api)
- [Project 2 â€” Financial Document Analyzer](#-project-2--financial-document-analyzer)
- [Project 3 â€” RAG Systems Essentials](#-project-3--rag-systems-essentials)
- [Project 4 â€” AI Web Research Agent](#-project-4--ai-web-research-agent)
- [Shared Technical Concepts](#-shared-technical-concepts)
- [Global Prerequisites](#-global-prerequisites)
- [Environment Variables Reference](#-environment-variables-reference)
- [Troubleshooting](#-troubleshooting)
- [License & Author](#-license--author)

---

## ğŸ¯ Repository Overview

This monorepo contains **four full-stack, independently deployable projects** organized by learning complexity:

| Level | Project | Domain | Core Technologies |
|:-----:|---------|--------|-------------------|
| **L2** | [Flask Stock Intelligence API](#-project-1--flask-stock-intelligence-api) | Financial Data & Analysis | Flask, yfinance, NumPy, Pandas |
| **L2** | [Financial Document Analyzer](#-project-2--financial-document-analyzer) | Document AI & OCR | FastAPI, Tesseract, Streamlit, Multi-LLM |
| **L2** | [RAG Systems Essentials](#-project-3--rag-systems-essentials) | Research Paper Q&A | FAISS, Sentence Transformers, Multi-LLM |
| **L3** | [AI Web Research Agent](#-project-4--ai-web-research-agent) | Autonomous Research | ReAct Pattern, Tavily, Multi-LLM |

### What Makes These Production-Grade

- **Separation of concerns** â€” Every project follows layered architecture (routes â†’ services â†’ utils)
- **Multi-LLM support** â€” Gemini, Groq, Ollama, and OpenAI/OpenRouter interchangeably across all AI projects
- **Input validation** â€” Comprehensive sanitization at every entry point
- **Structured error handling** â€” JSON error responses with appropriate HTTP status codes
- **Hallucination prevention** â€” Confidence gating, deterministic validation, constrained generation
- **Stateless design** â€” No server-side sessions; horizontally scalable

---

## ğŸ—ºï¸ Architecture Map

```
Pinnacle_Projects/
â”‚
â”œâ”€â”€ L2/                                    # Intermediate-level projects
â”‚   â”‚
â”‚   â”œâ”€â”€ Coding Essentials for Agent/
â”‚   â”‚   â””â”€â”€ flask-stock-api/               # PROJECT 1: REST API for stock intelligence
â”‚   â”‚       â”œâ”€â”€ run.py                     #   Entry point (port 5000)
â”‚   â”‚       â”œâ”€â”€ config.py                  #   Environment-based configuration
â”‚   â”‚       â””â”€â”€ app/
â”‚   â”‚           â”œâ”€â”€ __init__.py            #   Flask application factory
â”‚   â”‚           â”œâ”€â”€ routes/                #   Blueprint-based endpoint handlers
â”‚   â”‚           â”‚   â”œâ”€â”€ company.py         #     GET /api/company/<symbol>
â”‚   â”‚           â”‚   â”œâ”€â”€ stock.py           #     GET /api/stock/<symbol>
â”‚   â”‚           â”‚   â”œâ”€â”€ history.py         #     POST /api/history
â”‚   â”‚           â”‚   â””â”€â”€ analysis.py        #     POST /api/analyze
â”‚   â”‚           â”œâ”€â”€ services/              #   Business logic layer
â”‚   â”‚           â”‚   â”œâ”€â”€ yahoo_service.py   #     Yahoo Finance data fetching
â”‚   â”‚           â”‚   â””â”€â”€ analysis_service.py#     Quantitative computations
â”‚   â”‚           â””â”€â”€ utils/                 #   Validation & error handling
â”‚   â”‚               â”œâ”€â”€ errors.py          #     Custom exception classes
â”‚   â”‚               â””â”€â”€ validators.py      #     Input sanitization
â”‚   â”‚
â”‚   â”œâ”€â”€ Prompt Engineering Essentials/
â”‚   â”‚   â””â”€â”€ financial-document-analyzer/   # PROJECT 2: OCR + LLM document AI
â”‚   â”‚       â”œâ”€â”€ Jupiter NB/               #   Jupyter Notebook prototype
â”‚   â”‚       â”‚   â””â”€â”€ Financial_Report_Analysis.ipynb
â”‚   â”‚       â””â”€â”€ OCR-Tessaract/            #   Production system
â”‚   â”‚           â”œâ”€â”€ backend/              #     FastAPI backend (port 8000)
â”‚   â”‚           â”‚   â”œâ”€â”€ run.py            #       Entry point
â”‚   â”‚           â”‚   â””â”€â”€ app/
â”‚   â”‚           â”‚       â”œâ”€â”€ main.py       #       FastAPI routes + CORS
â”‚   â”‚           â”‚       â”œâ”€â”€ config.py     #       Pydantic settings (env vars)
â”‚   â”‚           â”‚       â”œâ”€â”€ models/       #       Pydantic schemas
â”‚   â”‚           â”‚       â””â”€â”€ services/     #       Core processing services
â”‚   â”‚           â”‚           â”œâ”€â”€ ocr_service.py        # Tesseract/PaddleOCR extraction
â”‚   â”‚           â”‚           â”œâ”€â”€ llm_service.py        # Multi-provider LLM abstraction
â”‚   â”‚           â”‚           â”œâ”€â”€ entity_extractor.py   # Deterministic financial NER
â”‚   â”‚           â”‚           â”œâ”€â”€ chart_analyzer.py     # Vision API chart interpretation
â”‚   â”‚           â”‚           â”œâ”€â”€ summarizer.py         # Role-aware report generation
â”‚   â”‚           â”‚           â””â”€â”€ table_extractor.py    # Table structure extraction
â”‚   â”‚           â”œâ”€â”€ frontend/             #     Streamlit UI (port 8501)
â”‚   â”‚           â”‚   â””â”€â”€ app.py            #       Upload, configure, visualize
â”‚   â”‚           â””â”€â”€ docs/                 #     Sample outputs
â”‚   â”‚
â”‚   â””â”€â”€ RAG Systems Essentials/           # PROJECT 3: Research paper Q&A
â”‚       â”œâ”€â”€ main.py                       #   Entry point
â”‚       â”œâ”€â”€ cli.py                        #   Interactive CLI commands
â”‚       â”œâ”€â”€ config.py                     #   Dataclass-based configuration
â”‚       â”œâ”€â”€ test_edge_cases.py            #   5 edge-case regression tests
â”‚       â”œâ”€â”€ src/
â”‚       â”‚   â”œâ”€â”€ document_processor.py     #   PDF extraction + semantic chunking
â”‚       â”‚   â”œâ”€â”€ embeddings.py             #   MiniLM-L6-v2 sentence embeddings
â”‚       â”‚   â”œâ”€â”€ vector_store.py           #   FAISS IndexFlatIP (cosine similarity)
â”‚       â”‚   â”œâ”€â”€ retriever.py              #   MMR + section-aware dual retrieval
â”‚       â”‚   â”œâ”€â”€ llm_providers.py          #   Gemini/Groq/Ollama provider manager
â”‚       â”‚   â””â”€â”€ rag_pipeline.py           #   Orchestration with confidence gating
â”‚       â””â”€â”€ data/
â”‚           â”œâ”€â”€ papers/                   #   Place PDF research papers here
â”‚           â””â”€â”€ index/                    #   Persisted FAISS index + chunk metadata
â”‚
â””â”€â”€ L3/                                   # Advanced-level project
    â””â”€â”€ Building AI Agents from Scratch/  # PROJECT 4: Autonomous research agent
        â”œâ”€â”€ agent.py                      #   Complete single-file agent (841 lines)
        â”œâ”€â”€ requirements.txt
        â””â”€â”€ reports/                      #   Auto-generated research reports (MD + HTML)
```

---

## ğŸ“ˆ Project 1 â€” Flask Stock Intelligence API

### Purpose

A **stateless, backend-only REST API** that provides real-time stock market intelligence â€” company metadata, live quotes, historical OHLCV data, and quantitative analysis â€” for integration with AI agents, trading bots, and dashboards.

### Technical Specifications

| Aspect | Detail |
|--------|--------|
| **Framework** | Flask 3.0+ with Application Factory pattern |
| **Data Source** | Yahoo Finance via `yfinance` 0.2.36+ |
| **Numerical Engine** | NumPy (statistical computations), Pandas (time-series operations) |
| **Production Server** | Gunicorn 21.0+ |
| **Port** | `5000` (development) |
| **Python Version** | 3.10+ |

### API Endpoints

| Method | Endpoint | Description | Parameters |
|--------|----------|-------------|------------|
| `GET` | `/health` | Health check | â€” |
| `GET` | `/api/company/<symbol>` | Company metadata (name, sector, officers, market cap) | Path: `symbol` |
| `GET` | `/api/stock/<symbol>` | Real-time quote (price, change, volume, 52-week range, market state) | Path: `symbol` |
| `POST` | `/api/history` | Historical OHLCV data | Body: `symbol`, `start_date`, `end_date`, `interval` |
| `POST` | `/api/analyze` | Quantitative analysis (volatility, trend, drawdown, insights) | Body: `symbol`, `start_date`, `end_date`, `interval` |

### Data Flow

```
Client Request
    â”‚
    â–¼
Route Blueprint â”€â”€â”€â”€ Input Validation (validators.py)
    â”‚                    â”‚
    â”‚              â—„â”€â”€â”€â”€â”€â”˜ (rejects invalid symbol/date/interval)
    â–¼
Service Layer
    â”œâ”€â”€ YahooFinanceService: get_company_info(), get_stock_data(), get_historical_data()
    â””â”€â”€ AnalysisService: analyze_stock()
            â”œâ”€â”€ _calculate_volatility()     â†’ Annualized std. dev. of daily returns (âˆš252)
            â”œâ”€â”€ _detect_trend()             â†’ Linear regression slope + start-end % comparison
            â”œâ”€â”€ _calculate_max_drawdown()   â†’ Peak-to-trough via running maximum
            â”œâ”€â”€ _calculate_return()         â†’ Total return percentage
            â””â”€â”€ _generate_insight()         â†’ Rule-based natural language summary
    â”‚
    â–¼
JSON Response (with structured error handling via custom exceptions)
```

### Analysis Algorithms

**Trend Detection** uses a dual-signal approach:
1. *Start-end percentage comparison* â€” classifies >5% as bullish/bearish
2. *Linear regression slope* via `numpy.polyfit(x, prices, 1)` â€” confirms direction

Combined classification: `bullish | mildly_bullish | sideways | mildly_bearish | bearish`

**Volatility** is computed as annualized standard deviation:

$$\sigma_{annual} = \sigma_{daily} \times \sqrt{252} \times 100$$

**Maximum Drawdown** uses the running-maximum method:

$$MDD = \min\left(\frac{P_t - \max_{s \leq t}(P_s)}{\max_{s \leq t}(P_s)}\right) \times 100$$

### Configuration

Three environment profiles via `config.py`:

| Profile | `DEBUG` | `TESTING` | Use Case |
|---------|---------|-----------|----------|
| `DevelopmentConfig` | `True` | `False` | Local development |
| `ProductionConfig` | `False` | `False` | Production deployment |
| `TestingConfig` | `True` | `True` | Automated tests |

Key settings: `YAHOO_TIMEOUT=10s`, `VALID_INTERVALS=['1d','1wk','1mo']`, `MAX_DATE_RANGE_DAYS=3650`, `MAX_HISTORY_RECORDS=5000`

### Quick Start

```powershell
cd "L2/Coding Essentials for Agent/flask-stock-api"
python -m venv venv
.\venv\Scripts\Activate.ps1
pip install -r requirements.txt
python run.py
# Server at http://localhost:5000
```

### Example Requests

```bash
# Company info
curl http://localhost:5000/api/company/AAPL

# Real-time quote
curl http://localhost:5000/api/stock/TSLA

# Historical data
curl -X POST http://localhost:5000/api/history \
  -H "Content-Type: application/json" \
  -d '{"symbol":"MSFT","start_date":"2024-01-01","end_date":"2024-12-31","interval":"1d"}'

# Quantitative analysis
curl -X POST http://localhost:5000/api/analyze \
  -H "Content-Type: application/json" \
  -d '{"symbol":"GOOGL","start_date":"2024-01-01","end_date":"2024-06-30"}'
```

---

## ğŸ“Š Project 2 â€” Financial Document Analyzer

### Purpose

An **AI-powered document intelligence system** that converts scanned financial documents (PDFs, images) into structured, role-aware financial summaries with hallucination-resistant architecture. Features a **FastAPI backend** and **Streamlit frontend**.

### Technical Specifications

| Aspect | Detail |
|--------|--------|
| **Backend Framework** | FastAPI 0.109 + Uvicorn ASGI server |
| **Frontend Framework** | Streamlit 1.31 |
| **OCR Engine** | Tesseract (primary) / PaddleOCR (optional) |
| **PDF Processing** | PyMuPDF (fitz) + pdfplumber |
| **Image Processing** | Pillow 10.2 + NumPy |
| **LLM Providers** | Gemini, Groq, Ollama, OpenAI (switchable per request) |
| **Vision Model** | Gemini Vision (chart/graph interpretation) |
| **Validation** | Pydantic 2.5 (schemas) + Pydantic-Settings (env config) |
| **Ports** | Backend: `8000`, Frontend: `8501` |

### Architecture â€” Hallucination-Resistant Pipeline

The system's key innovation is separating **deterministic data extraction** from **LLM reasoning**:

```
Document Upload (PDF/Image/JSON)
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Stage 1: Image Preprocessing    â”‚  Pillow: enhancement, DPI normalization (300 DPI)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Stage 2: Text Extraction        â”‚  pdfplumber (tables) + Tesseract OCR (text)
â”‚                                  â”‚  Returns: OCRBlock[] with confidence & bounding boxes
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Stage 3: Chart Analysis         â”‚  Gemini Vision API detects chart type, trends,
â”‚  (Optional)                      â”‚  key values from visual graphs
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Stage 4: Deterministic NER      â”‚  entity_extractor.py: 100+ regex patterns for
â”‚  (CODE decides, NOT LLM)         â”‚  revenue, profit, EPS, ratios, currency, period
â”‚                                  â”‚  detection (Q1-Q4, FY, YTD), YoY/QoQ changes
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Stage 5: Period-Keyed JSON      â”‚  Structured source-of-truth with latest_period,
â”‚  (Source of Truth)               â”‚  earliest_period, all extracted metrics
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Stage 6: LLM Report Generation  â”‚  Single LLM call with pre-templated values
â”‚  (Role-Aware)                    â”‚  Role: Investor | Analyst | Auditor | Executive
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
  Structured Summary + Confidence Score (JSON/Markdown export)
```

### Core Services

| Service | File | Responsibility |
|---------|------|----------------|
| **OCRService** | `ocr_service.py` | Tesseract/PaddleOCR text extraction with block-level confidence scoring, bounding box detection, auto-detects Tesseract on Windows |
| **LLMService** | `llm_service.py` | Abstract base class with `GeminiProvider`, `GroqProvider`, `OllamaProvider` implementations; generates `LLMResponse` with token accounting |
| **FinancialEntityExtractor** | `entity_extractor.py` | 100+ compiled regex patterns for currency (8 symbols), periods (Q/FY/H/YTD/TTM), metrics (revenue, profit, EPS, ratios), change detection (YoY/QoQ/MoM), value normalization (mn/bn/cr/lakh) |
| **ChartAnalyzer** | `chart_analyzer.py` | Vision-LLM chart interpretation: detects `BAR|LINE|PIE|AREA|STACKED_BAR|COMBO|WATERFALL` chart types, extracts `UP|DOWN|STABLE|VOLATILE` trends |
| **TableExtractor** | `table_extractor.py` | Structural table detection from OCR blocks |
| **FinancialSummarizer** | `summarizer.py` | Orchestrates all services, assembles role-aware prompts |

### Role-Aware Summarization

The LLM adapts its output based on the selected user role:

| Role | Focus Areas | Key Metrics | Tone |
|------|-------------|-------------|------|
| **Investor** | Growth potential, profitability, risk | Revenue Growth, NPM, EPS, ROE, P/E | Decision-oriented |
| **Analyst** | Ratios, trends, valuation | ROE, ROA, D/E, Current Ratio, Op. Margin | Technical, data-driven |
| **Auditor** | Compliance, anomalies, red flags | Asset Quality, CAR, CET1, NPL | Scrutinizing |
| **Executive** | Strategy, competitive position | Revenue, EBITDA, Market Share | High-level, strategic |

### API Endpoints

| Method | Endpoint | Description |
|--------|----------|-------------|
| `GET` | `/` | Health check with service status |
| `GET` | `/health` | Detailed health check (LLM + Vision availability) |
| `GET` | `/providers` | List available LLM providers and their status |
| `POST` | `/analyze` | Upload document + role + provider â†’ structured analysis |

### Quick Start

```powershell
# Terminal 1: Backend
cd "L2/Prompt Engineering Essentials/financial-document-analyzer/OCR-Tessaract/backend"
python -m venv venv
.\venv\Scripts\Activate.ps1
pip install -r requirements.txt
python run.py
# Backend at http://localhost:8000 | Docs at http://localhost:8000/docs

# Terminal 2: Frontend
cd "L2/Prompt Engineering Essentials/financial-document-analyzer/OCR-Tessaract/frontend"
pip install -r requirements.txt
streamlit run app.py --server.port 8501
# Frontend at http://localhost:8501
```

### System Requirements

- **Tesseract OCR** must be installed separately:
  - **Windows**: Download from [UB-Mannheim/tesseract](https://github.com/UB-Mannheim/tesseract/wiki)
  - **macOS**: `brew install tesseract`
  - **Linux**: `sudo apt-get install tesseract-ocr`
- At least **one LLM provider** configured (see [Environment Variables](#-environment-variables-reference))

---

## ğŸ” Project 3 â€” RAG Systems Essentials

### Purpose

A **portfolio-grade Retrieval-Augmented Generation system** for question-answering over AI research papers, featuring confidence gating, hallucination prevention, section-aware retrieval, and multi-LLM support. Achieves **5/5 (Grade A)** on edge-case tests.

### Technical Specifications

| Aspect | Detail |
|--------|--------|
| **Embedding Model** | `sentence-transformers/all-MiniLM-L6-v2` (384 dimensions) |
| **Vector Store** | FAISS `IndexFlatIP` (cosine similarity via normalized inner product) |
| **Retrieval Strategy** | Dual-strategy: MMR for diversity + section-aware boosting |
| **LLM Providers** | Gemini, Groq, Ollama (**no OpenAI dependency**) |
| **PDF Processing** | PyMuPDF (primary) + pdfminer.six (backup) |
| **Python Version** | 3.9+ |

### Architecture Pipeline

```
User Question
     â”‚
     â”œâ”€â”€â”€ Query Type Detection
     â”‚    (factual | table | section | cross-paper)
     â”‚
     â”œâ”€â”€â”€ Query Encoding (MiniLM-L6-v2, 384-dim)
     â”‚
     â”œâ”€â”€â”€ FAISS Retrieval (top_k=6)
     â”‚    â”œâ”€â”€ Maximal Marginal Relevance (Î»=0.3)
     â”‚    â””â”€â”€ Section-Aware Score Boosting:
     â”‚         abstract=1.2x, introduction=1.1x,
     â”‚         architecture=1.3x, method=1.25x
     â”‚
     â”œâ”€â”€â”€ Confidence Gate
     â”‚    â”œâ”€â”€ General queries:  threshold = 0.50
     â”‚    â”œâ”€â”€ Factual queries:  threshold = 0.75
     â”‚    â””â”€â”€ Below threshold â†’ "This information is not present..."
     â”‚
     â”œâ”€â”€â”€ LLM Generation (query-type-specific prompts)
     â”‚    â”œâ”€â”€ DEFAULT_SYSTEM_PROMPT      â†’ conceptual questions
     â”‚    â”œâ”€â”€ FACTUAL_SYSTEM_PROMPT      â†’ exact values/numbers
     â”‚    â”œâ”€â”€ CROSS_PAPER_SYSTEM_PROMPT  â†’ multi-paper synthesis
     â”‚    â””â”€â”€ SECTION_QUERY_SYSTEM_PROMPT â†’ structural questions
     â”‚
     â””â”€â”€â”€ Answer + Source Citations (max 2)
```

### Core Components

| Component | File | Technical Detail |
|-----------|------|------------------|
| **DocumentProcessor** | `document_processor.py` | PDF extraction with semantic chunking (`chunk_size=400 tokens`, `overlap=75 tokens`, `min=100 tokens`) |
| **EmbeddingModel** | `embeddings.py` | Sentence Transformers wrapper with batch encoding (`batch_size=32`) |
| **FAISSVectorStore** | `vector_store.py` | FAISS `IndexFlatIP` with L2 normalization for cosine similarity, persistence to disk (JSON metadata + `.index` file) |
| **Retriever** | `retriever.py` | Query-type detection via keyword matching (`FACTUAL_KEYWORDS`, `TABLE_KEYWORDS`, `SECTION_KEYWORDS`), MMR diversity, section-aware score boosting |
| **RAGPipeline** | `rag_pipeline.py` | Full orchestration with 4 specialized system prompts, confidence-gated generation, structured `RAGResponse` output with timing metrics |
| **LLMManager** | `llm_providers.py` | Multi-provider abstraction for Gemini, Groq, Ollama |

### Hallucination Prevention (5 Layers)

| Layer | Mechanism | Implementation |
|-------|-----------|----------------|
| 1 | **Confidence Gating** | Refuses when retrieval confidence < threshold (0.50 general / 0.75 factual) |
| 2 | **Strict System Prompts** | "Answer ONLY from context" instruction in all 4 prompt templates |
| 3 | **Citation Discipline** | Max 2 citations, filtered to query-relevant papers only |
| 4 | **Table Query Detection** | Extra-strict 0.75 threshold for numeric/table data requests |
| 5 | **Cross-Paper Isolation** | Prevents data leakage between separate papers |

### Configuration Reference (`config.py`)

```python
# Chunking
chunk_size = 400          # tokens per chunk
chunk_overlap = 75        # overlap tokens
min_chunk_size = 100      # minimum tokens

# Embedding
model_name = "sentence-transformers/all-MiniLM-L6-v2"
dimension = 384

# Retrieval
top_k = 6                 # chunks to retrieve
similarity_threshold = 0.3
use_mmr = True
mmr_diversity = 0.3       # Î» (0=max diversity, 1=max relevance)

# Confidence Thresholds
CONFIDENCE_THRESHOLD = 0.50
FACTUAL_CONFIDENCE_THRESHOLD = 0.75

# Token Limits by Query Type
cross_paper_max_tokens = 256
factual_max_tokens = 150
general_max_tokens = 1024
```

### Edge Case Test Results

| Test | What It Validates | Result |
|------|-------------------|--------|
| Table Exactness | Refuses to hallucinate missing table data | âœ… PASS |
| Negative Refusal | Clean refusal with proper citations | âœ… PASS |
| Cross-Paper Reasoning | Concise synthesis without extra metrics | âœ… PASS |
| Section Precision | Identifies Section 3.2 for multi-head attention | âœ… PASS |
| Knowledge Boundaries | No data leakage across papers | âœ… PASS |

### Quick Start

```powershell
cd "L2/RAG Systems Essentials"
python -m venv venv
.\venv\Scripts\Activate.ps1
pip install -r requirements.txt

# Place PDFs in data/papers/
# Then run:
python main.py --provider ollama
# Or single question:
python main.py -q "What is multi-head attention?"
# Run tests:
python test_edge_cases.py
```

### CLI Commands

| Command | Description |
|---------|-------------|
| `/ingest` | Ingest PDFs from `data/papers/` into vector index |
| `/stats` | Show index statistics (chunk count, papers indexed) |
| `/provider <name>` | Switch LLM provider at runtime |
| `/help` | Show available commands |
| `/quit` | Exit |

---

## ğŸ”¬ Project 4 â€” AI Web Research Agent

### Purpose

An **autonomous research agent** implementing the **ReAct (Reason + Act) pattern** that automates end-to-end web research: generating research questions, searching the web, synthesizing findings, and producing formatted reports in both Markdown and HTML.

### Technical Specifications

| Aspect | Detail |
|--------|--------|
| **Design Pattern** | ReAct (Reason + Act) â€” alternating LLM reasoning and tool use |
| **Web Search** | Tavily API (`search_depth="advanced"`, 3 results/question) |
| **LLM Providers** | Gemini, Groq, Ollama, OpenRouter (4 providers) |
| **CLI Framework** | Rich (spinners, panels, progress bars, colored output) |
| **Output Formats** | Markdown (`.md`) + styled HTML (`.html`, auto-opens in browser) |
| **Architecture** | Single-file agent (`agent.py`, 841 lines) with clean class hierarchy |

### ReAct Execution Flow

```
User Topic: "AI in Healthcare"
â”‚
â”œâ”€â”€ PHASE 1: PLANNING (Reason)
â”‚   LLM generates 4 targeted research questions
â”‚   JSON output: ["Q1?", "Q2?", "Q3?", "Q4?"]
â”‚   Fallback: Template-based question generation on JSON parse failure
â”‚
â”œâ”€â”€ PHASE 2: ACTING (Act)
â”‚   For each question â†’ Tavily Web Search (advanced depth)
â”‚   Returns: SearchResult(title, url, content, score) Ã— 3 per question
â”‚   Total: ~12 web results gathered
â”‚
â”œâ”€â”€ PHASE 3: SYNTHESIS (Reason)
â”‚   For each question â†’ LLM summarizes search results
â”‚   Input truncated to 300 chars/result for token efficiency
â”‚   Output: Markdown bullet-point summaries (150-200 words each)
â”‚
â”œâ”€â”€ PHASE 4: FRAMING (Reason)
â”‚   LLM generates introduction (50-80 words) and conclusion (50-80 words)
â”‚
â””â”€â”€ PHASE 5: REPORT GENERATION
    â”œâ”€â”€ Markdown report with TOC, 4-6 sections, source citations
    â”œâ”€â”€ HTML conversion with styled template (CSS custom properties)
    â””â”€â”€ Auto-opens HTML in default browser
```

### Class Hierarchy

```
LLMProvider (ABC)               # Abstract base for all LLM providers
â”œâ”€â”€ GeminiProvider              #   google.genai SDK, exponential backoff (3 retries)
â”œâ”€â”€ GroqProvider                #   groq SDK, chat completions
â”œâ”€â”€ OllamaProvider              #   ollama SDK, local inference
â””â”€â”€ OpenRouterProvider          #   HTTP REST API, supports 50+ models

WebSearchTool                   # Tavily API wrapper
â”œâ”€â”€ search()                    #   Returns List[SearchResult]

ResearchAgent                   # Main orchestrator
â”œâ”€â”€ plan()                      #   REASON: Generate research questions (JSON)
â”œâ”€â”€ act()                       #   ACT: Execute web searches
â”œâ”€â”€ reason()                    #   REASON: Synthesize single question's results
â”œâ”€â”€ synthesize_all()            #   REASON: Batch synthesis with progress bars
â”œâ”€â”€ generate_introduction()     #   REASON: Create report intro
â”œâ”€â”€ generate_conclusion()       #   REASON: Create report conclusion
â”œâ”€â”€ generate_report()           #   Assemble final Markdown
â”œâ”€â”€ _generate_filename()        #   LLM-generated meaningful filename
â”œâ”€â”€ _save_and_open_html()       #   Markdownâ†’HTML conversion + browser open
â””â”€â”€ research()                  #   Full workflow orchestration
```

### Key Design Decisions

| Decision | Rationale |
|----------|-----------|
| Content truncation (300 chars) | Optimizes for free-tier API token limits |
| 4 questions (configurable) | Balances coverage vs. API costs |
| Exponential backoff (Gemini) | Handles rate limiting (429/RESOURCE_EXHAUSTED) gracefully |
| Separate intro/conclusion LLM calls | Keeps each prompt focused and token-efficient |
| `rich` CLI library | Professional terminal UX with spinners, panels, progress indicators |

### Quick Start

```powershell
cd "L3/Building AI Agents from Scratch"
python -m venv venv
.\venv\Scripts\Activate.ps1
pip install -r requirements.txt

# Create .env with at least TAVILY_API_KEY + one LLM provider key
# Then:
python agent.py "Artificial Intelligence in Healthcare"

# Custom output:
python agent.py "Climate Change" -o climate_report.md

# Override provider:
python agent.py "Quantum Computing" --provider groq
```

---

## ğŸ”— Shared Technical Concepts

### Multi-LLM Provider Pattern

All three AI projects (Projects 2, 3, 4) implement the same provider abstraction:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        Abstract LLM Interface         â”‚
â”‚  generate(prompt, system_prompt) â†’ strâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â–²         â–²         â–²         â–²
         â”‚         â”‚         â”‚         â”‚
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â” â”Œâ”€â”€â”€â”´â”€â”€â”€â” â”Œâ”€â”€â”´â”€â”€â”€â”€â” â”Œâ”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Gemini â”‚ â”‚  Groq â”‚ â”‚Ollama â”‚ â”‚ OpenRouter â”‚
    â”‚ (Cloud)â”‚ â”‚(Cloud)â”‚ â”‚(Local)â”‚ â”‚  (Cloud)   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

| Provider | Type | Speed | Cost | Best For |
|----------|------|-------|------|----------|
| **Ollama** | Local | Medium | Free | Development, privacy |
| **Groq** | Cloud | Fastest | Free tier | Speed-critical tasks |
| **Gemini** | Cloud | Fast | Free tier | Good balance |
| **OpenAI** | Cloud | Fast | Paid | Best quality |
| **OpenRouter** | Cloud | Varies | Pay-per-use | Model variety |

### Shared Design Patterns

| Pattern | Used In | Implementation |
|---------|---------|----------------|
| Application Factory | Project 1 | `create_app()` in Flask `__init__.py` |
| Blueprint/Router | Projects 1, 2 | Flask Blueprints / FastAPI routers |
| Service Layer | All | Business logic separated from routes |
| Abstract Base Class | Projects 2, 3, 4 | `BaseLLMProvider(ABC)` / `LLMProvider(ABC)` |
| Factory Function | Project 4 | `get_llm_provider()` returns configured instance |
| Dataclass Models | Projects 3, 4 | `@dataclass` for structured data (RAGResponse, SearchResult) |
| Pydantic Settings | Project 2 | Environment variable configuration with validation |

---

## âš™ï¸ Global Prerequisites

| Requirement | Version | Required By | Notes |
|-------------|---------|-------------|-------|
| **Python** | 3.10+ | All projects | 3.9+ for RAG only |
| **pip** | Latest | All projects | Package manager |
| **Tesseract OCR** | Latest | Project 2 | System-level install |
| **Internet** | â€” | All (except Ollama) | API access |
| **Git** | Latest | â€” | Optional, for cloning |

### Recommended: Virtual Environments

Each project should use its own virtual environment to avoid dependency conflicts:

```powershell
# Create and activate per-project
cd "L2/Coding Essentials for Agent/flask-stock-api"
python -m venv venv
.\venv\Scripts\Activate.ps1
pip install -r requirements.txt
```

---

## ğŸ” Environment Variables Reference

Create a `.env` file in each project that uses LLMs:

```env
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  LLM PROVIDER SELECTION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
LLM_PROVIDER=gemini               # gemini | groq | ollama | openrouter

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  GOOGLE GEMINI
#  Get key: https://makersuite.google.com/app/apikey
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
GEMINI_API_KEY=your-gemini-key
GEMINI_MODEL=gemini-2.0-flash     # or gemini-1.5-pro

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  GROQ (Fast Inference)
#  Get key: https://console.groq.com/keys
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
GROQ_API_KEY=gsk_your-key
GROQ_MODEL=llama-3.3-70b-versatile

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  OLLAMA (Local, Free)
#  Install: https://ollama.ai â†’ ollama pull llama3.2
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  OPENROUTER (Multi-Model, Project 4 only)
#  Get key: https://openrouter.ai/keys
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
OPENROUTER_API_KEY=your-key
OPENROUTER_MODEL=google/gemma-2-9b-it:free

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  OPENAI (Project 2 only)
#  Get key: https://platform.openai.com/api-keys
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
OPENAI_API_KEY=sk-your-key
OPENAI_MODEL=gpt-4o

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  TAVILY WEB SEARCH (Project 4 only)
#  Get key: https://tavily.com (1000 searches/month free)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
TAVILY_API_KEY=tvly-your-key

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  VISION MODEL (Project 2 â€” Chart Analysis)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
VISION_PROVIDER=gemini
VISION_MODEL=gemini-1.5-flash
```

---

## ğŸ”§ Troubleshooting

| Problem | Project | Solution |
|---------|---------|----------|
| `ModuleNotFoundError` | Any | Ensure virtual environment is activated and `pip install -r requirements.txt` ran |
| `TesseractNotFoundError` | 2 | Install Tesseract system binary; set `TESSERACT_CMD` in `.env` if non-standard path |
| `RESOURCE_EXHAUSTED` / 429 | 2, 3, 4 | Rate limited â€” wait 60s, switch to Ollama (`--provider ollama`), or use Groq |
| `yfinance returns None` | 1 | Check internet connection; symbol may be delisted or invalid |
| Low confidence / refusals | 3 | Expected behavior â€” confidence gating rejects uncertain answers. Add more relevant PDFs to `data/papers/` |
| `TAVILY_API_KEY` error | 4 | Sign up at [tavily.com](https://tavily.com) for free key (1000/month) |
| No charts detected | 2 | Requires `GEMINI_API_KEY` for Vision model; text analysis still works without it |
| FAISS import error | 3 | Install with `pip install faiss-cpu` (not `faiss`) |
| Ollama not responding | 2, 3, 4 | Run `ollama serve` in a separate terminal; pull model with `ollama pull llama3.2` |

---

## ğŸ“„ License & Author

**License:** MIT â€” Use freely for educational and portfolio purposes.

**Author:** AHILL S

---

<div align="center">

*Built with dedication to production-grade software engineering, clean architecture, and responsible AI design.*

</div>